//go:generate go run . -out ../../asm_amd64.s -stubs ../../stubs_amd64.go -pkg vectorcmp --dispatcher-amd64 ../../dispatch_amd64.go --dispatcher-other ../../dispatch_other.go
package main

import (
	"bytes"
	"flag"
	"fmt"
	"go/format"
	"io"
	"log"
	"os"

	. "github.com/mmcloughlin/avo/build"
	. "github.com/mmcloughlin/avo/operand"
	"github.com/mmcloughlin/avo/reg"
)

var constants = GLOBL("constants", RODATA|NOPTR)

var (
	dispatcherAmd64File = flag.String("dispatcher-amd64", "", "File name to write dispatcher functions for amd64 to")
	dispatcherOtherFile = flag.String("dispatcher-other", "", "File name to write dispatcher functions for non-amd64 to")

	dispatcherAmd64 bytes.Buffer
	dispatcherOther bytes.Buffer
	dispatcherBoth  = io.MultiWriter(&dispatcherAmd64, &dispatcherOther)
)

type CmpOp string

var (
	Equals        CmpOp = "Equals"
	GreaterThan   CmpOp = "GreaterThan"
	LessThan      CmpOp = "LessThan"
	GreaterEquals CmpOp = "GreaterEquals"
	LesserEquals  CmpOp = "LesserEquals"
)

func main() {
	DATA(0, U32(0b01010101010101010101010101010101))
	DATA(4, U32(0b00010001000100010001000100010001))
	DATA(8, U32(0b00000001000000010000000100000001))
	ConstraintExpr("!purego")

	io.WriteString(dispatcherBoth, "// Code generated by avo.go. DO NOT EDIT.\n\n")
	dispatcherAmd64.WriteString("//go:build amd64 && !purego\n\n")
	dispatcherOther.WriteString("//go:build !amd64 || purego\n\n")
	io.WriteString(dispatcherBoth, "package vectorcmp\n\n")

	for _, w := range []int{8, 16, 32, 64} {
		for _, o := range []CmpOp{Equals, GreaterThan, LessThan, GreaterEquals, LesserEquals} {
			fastFilter(w, o)
		}
	}

	Generate()

	for _, d := range []struct {
		fn  string
		src *bytes.Buffer
	}{{*dispatcherAmd64File, &dispatcherAmd64}, {*dispatcherOtherFile, &dispatcherOther}} {
		if d.fn == "" {
			continue
		}
		src, err := format.Source(d.src.Bytes())
		if err != nil {
			log.Fatalf("Failed to format generated Go code: %v", err)
		}
		if err := os.WriteFile(d.fn, src, 0644); err != nil {
			log.Fatalf("Failed to write to %q: %v", d.fn, err)
		}
	}
}

type GPOp interface {
	Op
	reg.GP
}

func fastFilter(width int, cmpOp CmpOp) {
	const rounds = 2

	fmt.Fprintf(dispatcherBoth, "func Vector%s%d(dstMask []byte, b uint%d, rows []uint%d) {\n", cmpOp, width, width, width)
	fmt.Fprintf(&dispatcherAmd64, "	if hasAVX2() && len(rows) >= %d {\n", 256*rounds/width)
	fmt.Fprintf(&dispatcherAmd64, "		asmBulk%s%d(dstMask, b, rows[:len(rows) & ^%d])\n", cmpOp, width, 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		dstMask = dstMask[(len(rows) & ^%d) / 8:]\n", 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "		rows = rows[len(rows) & ^%d:]\n", 256*rounds/width-1)
	fmt.Fprintf(&dispatcherAmd64, "	}\n")
	fmt.Fprintf(dispatcherBoth, "	goVector%s(dstMask, b, rows)\n", cmpOp)
	fmt.Fprintf(dispatcherBoth, "}\n")

	TEXT(fmt.Sprintf("asmBulk%s%d", cmpOp, width), NOSPLIT, fmt.Sprintf("func(dstMask []byte, b uint%d, rows []uint%d)", width, width))
	Pragma("noescape")

	dstMask := Load(Param("dstMask").Base(), GP64())
	rows := Load(Param("rows").Base(), GP64())
	l := Load(Param("rows").Len(), GP64())

	bRepeated := YMM()
	var ymms []Op
	var intermediates []GPOp
	for i := 0; rounds > i; i++ {
		ymms = append(ymms, YMM())
		intermediates = append(intermediates, GP32())
	}

	Comment("Read param b into YMM register. If b is 0x07, YMM becomes {0x07, 0x07, 0x07...}")
	b, err := Param("b").Resolve()
	if err != nil {
		panic(err)
	}
	switch width {
	case 8:
		VPBROADCASTB(b.Addr, bRepeated)
	case 16:
		VPBROADCASTW(b.Addr, bRepeated)
	case 32:
		VPBROADCASTD(b.Addr, bRepeated)
	case 64:
		VPBROADCASTQ(b.Addr, bRepeated)
	}

	mask := GP32()
	switch width {
	case 8:
	case 16:
		Comment("Load the mask 01010101... which we will use with PEXT to drop half the bits")
		MOVL(constants.Offset(0), mask)
	case 32:
		Comment("Load the mask 00010001... which we will use with PEXT to drop 75% of the bits")
		MOVL(constants.Offset(4), mask)
	case 64:
		Comment("Load the mask 00000001... which we will use with PEXT to drop 7/8th of the bits")
		MOVL(constants.Offset(8), mask)
	}

	Label("loop")

	Commentf("Load %d 256-bit chunks into YMM registers", rounds)
	for i := 0; i < rounds; i++ {
		VMOVDQU(Mem{Base: rows, Disp: 32 * i}, ymms[i])
	}
	Comment("Compare all bytes in each YMM register to b. Each byte in the YMMs becomes 0x00 (mismatch) or 0xff (match)")
	for i := 0; i < rounds; i++ {
		var instr func(ops ...Op)
		switch width {
		case 8:
			if cmpOp == Equals {
				instr = VPCMPEQB
			} else {
				instr = VPCMPGTB
			}
		case 16:
			if cmpOp == Equals {
				instr = VPCMPEQW
			} else {
				instr = VPCMPGTW
			}
		case 32:
			if cmpOp == Equals {
				instr = VPCMPEQD
			} else {
				instr = VPCMPGTD
			}
		case 64:
			if cmpOp == Equals {
				instr = VPCMPEQQ
			} else {
				instr = VPCMPGTQ
			}
		}
		args := []Op{ymms[i], bRepeated, ymms[i]}
		switch cmpOp {
		case LessThan, GreaterEquals:
			// Flip the arguments
			args[0], args[1] = args[1], args[0]
		}
		instr(args...)
	}
	Comment("Take one bit of each byte and pack it into an R32")
	for i := 0; i < rounds; i++ {
		VPMOVMSKB(ymms[i], intermediates[i])
	}
	switch width {
	case 8:
	case 16:
		Comment("Drop every second bit from these registers")
		for i := 0; i < rounds; i++ {
			PEXTL(mask, intermediates[i], intermediates[i])
		}
	case 32:
		Comment("Drop every second-fourth bit from these registers")
		for i := 0; i < rounds; i++ {
			PEXTL(mask, intermediates[i], intermediates[i])
		}
	case 64:
		Comment("Drop every second-seventh bit from these registers")
		for i := 0; i < rounds; i++ {
			PEXTL(mask, intermediates[i], intermediates[i])
		}
	}
	implementedWithInversion := cmpOp == GreaterEquals || cmpOp == LesserEquals
	if implementedWithInversion {
		Commentf("To get %s semantics, we flipped the arguments of VPCMPGT and now invert the result", cmpOp)
		for _, r := range intermediates {
			switch width {
			case 8:
				NOTL(r)
			case 16:
				NOTW(r.As16())
			case 32:
				NOTB(r.As8())
			}
		}
	}
	Comment("Write the registers to dstMask")
	for i, r := range intermediates {
		switch width {
		case 8:
			MOVL(r, Mem{Base: dstMask, Disp: 4 * i})
		case 16:
			MOVW(r.As16(), Mem{Base: dstMask, Disp: 2 * i})
		case 32:
			MOVB(r.As8(), Mem{Base: dstMask, Disp: 1 * i})
		}
	}
	if width == 64 {
		Comment("Each register contains 4 bytes, so we first combine pairs before writing them back")
		for i := 0; rounds/2 > i; i++ {
			SHLB(U8(4), intermediates[2*i+1].As8())
			ORB(intermediates[2*i+1].As8(), intermediates[2*i+0].As8())
			if implementedWithInversion {
				NOTB(intermediates[2*i+0].As8())
			}
			MOVB(intermediates[2*i+0].As8(), Mem{Base: dstMask, Disp: 1 * i})
		}
	}

	Comment("Update our offsets into rows and dstMask")
	ADDQ(U32(rounds*32), rows)
	ADDQ(U32(rounds*32/width), dstMask)

	Comment("Decrement loop counter")
	SUBQ(U32(rounds*256/width), l)
	JNZ(LabelRef("loop"))
	RET()
}
